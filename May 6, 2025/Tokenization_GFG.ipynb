{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzR_IsxIrBbL",
        "outputId": "7bead19d-3413-434b-c8eb-1700c4c1f9db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLqTzjPHrSnh",
        "outputId": "e99d9086-26c8-4e23-ba0b-b7cd1f6b6cd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZbpv3vKqILD",
        "outputId": "c8781586-d081-49c3-9752-ccd3d52dba23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Smith graduated from Harvard in 1999.',\n",
              " 'Can you believe it?',\n",
              " \"He said, 'I'm going to change the world!'\",\n",
              " 'And indeed, he did.',\n",
              " \"Now he's known globally‚Äîas a leader, a thinker, and an innovator.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"\"\"Dr. Smith graduated from Harvard in 1999. Can you believe it?\n",
        "He said, 'I'm going to change the world!' And indeed, he did.\n",
        "Now he's known globally‚Äîas a leader, a thinker, and an innovator.\n",
        "\"\"\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to train a PunkTrainer to train in Telugu."
      ],
      "metadata": {
        "id": "LR04rVMQtkvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.punkt import PunktTrainer, PunktSentenceTokenizer\n",
        "\n",
        "# Step 1: Prepare training text (Telugu)\n",
        "telugu_text = \"\"\"\n",
        "‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã‡∞®‡∞ø 29 ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã ‡∞í‡∞ï‡∞ü‡∞ø.\n",
        "‡∞á‡∞¶‡∞ø ‡∞¶‡∞ï‡±ç‡∞∑‡∞ø‡∞£ ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞π‡±à‡∞¶‡∞∞‡∞æ‡∞¨‡∞æ‡∞¶‡±ç ‡∞®‡∞ó‡∞∞‡∞Ç ‡∞à ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞∞‡∞æ‡∞ú‡∞ß‡∞æ‡∞®‡∞ø.\n",
        "‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞Ö‡∞®‡±á‡∞¶‡∞ø ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã ‡∞™‡∞¶‡∞µ ‡∞Ö‡∞§‡∞ø ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç.\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Train a Punkt tokenizer\n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(telugu_text)\n",
        "\n",
        "print(trainer.get_params())\n",
        "\n",
        "# Step 3: Create tokenizer from the trainer\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
        "\n",
        "# Step 4: Tokenize Telugu text\n",
        "sentences = tokenizer.tokenize(telugu_text)\n",
        "\n",
        "# Output\n",
        "for i, s in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {s}\")\n",
        "\n",
        "random_text=\"‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç ‡∞í‡∞ï ‡∞µ‡∞ø‡∞∂‡∞æ‡∞≤‡∞Æ‡±à‡∞® ‡∞¶‡±á‡∞∂‡∞Ç. ‡∞á‡∞¶‡∞ø ‡∞≠‡∞ø‡∞®‡±ç‡∞®‡∞Æ‡±à‡∞® ‡∞≠‡∞æ‡∞∑‡∞≤‡±Å, ‡∞∏‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡±Å‡∞≤‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞π‡∞ø‡∞Æ‡∞æ‡∞≤‡∞Ø‡∞æ‡∞≤‡±Å ‡∞â‡∞§‡±ç‡∞§‡∞∞‡∞Ç‡∞≤‡±ã ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞®‡∞¶‡∞ø ‡∞ó‡±ä‡∞™‡±ç‡∞™ ‡∞®‡∞¶‡∞ø. ‡∞á‡∞¶‡∞ø ‡∞é‡∞Ç‡∞§‡±ã ‡∞™‡∞µ‡∞ø‡∞§‡±ç‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞™‡∞∞‡∞ø‡∞ó‡∞£‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.\"\n",
        "\n",
        "tokenizer.tokenize(random_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84SearfsaPO",
        "outputId": "f69a00f0-72de-4e5c-b898-cc7086cc9b96",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<nltk.tokenize.punkt.PunktParameters object at 0x79aefa28d910>\n",
            "Sentence 1: \n",
            "‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã‡∞®‡∞ø 29 ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã ‡∞í‡∞ï‡∞ü‡∞ø.\n",
            "Sentence 2: ‡∞á‡∞¶‡∞ø ‡∞¶‡∞ï‡±ç‡∞∑‡∞ø‡∞£ ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞¶‡∞ø.\n",
            "Sentence 3: ‡∞π‡±à‡∞¶‡∞∞‡∞æ‡∞¨‡∞æ‡∞¶‡±ç ‡∞®‡∞ó‡∞∞‡∞Ç ‡∞à ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞∞‡∞æ‡∞ú‡∞ß‡∞æ‡∞®‡∞ø.\n",
            "Sentence 4: ‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞Ö‡∞®‡±á‡∞¶‡∞ø ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã ‡∞™‡∞¶‡∞µ ‡∞Ö‡∞§‡∞ø ‡∞™‡±Ü‡∞¶‡±ç‡∞¶ ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç ‡∞í‡∞ï ‡∞µ‡∞ø‡∞∂‡∞æ‡∞≤‡∞Æ‡±à‡∞® ‡∞¶‡±á‡∞∂‡∞Ç.',\n",
              " '‡∞á‡∞¶‡∞ø ‡∞≠‡∞ø‡∞®‡±ç‡∞®‡∞Æ‡±à‡∞® ‡∞≠‡∞æ‡∞∑‡∞≤‡±Å, ‡∞∏‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡±Å‡∞≤‡±Å ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø.',\n",
              " '‡∞π‡∞ø‡∞Æ‡∞æ‡∞≤‡∞Ø‡∞æ‡∞≤‡±Å ‡∞â‡∞§‡±ç‡∞§‡∞∞‡∞Ç‡∞≤‡±ã ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø.',\n",
              " '‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞®‡∞¶‡∞ø ‡∞ó‡±ä‡∞™‡±ç‡∞™ ‡∞®‡∞¶‡∞ø.',\n",
              " '‡∞á‡∞¶‡∞ø ‡∞é‡∞Ç‡∞§‡±ã ‡∞™‡∞µ‡∞ø‡∞§‡±ç‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞™‡∞∞‡∞ø‡∞ó‡∞£‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"‡∞®‡∞æ ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡∞∞‡∞ø‡∞∑‡±ç‡∞Æ‡∞æ. ‡∞®‡±á‡∞®‡±Å ‡∞Ü‡∞Ç‡∞ß‡±ç‡∞∞‡∞™‡±ç‡∞∞‡∞¶‡±á‡∞∂‡±ç‚Äå‡∞≤‡±ã‡∞®‡∞ø ‡∞®‡±Ü‡∞≤‡±ç‡∞≤‡±Ç‡∞∞‡±Å ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞µ‡∞ö‡±ç‡∞ö‡∞æ‡∞®‡±Å. ‡∞™‡±ç‡∞∞‡∞∏‡±ç‡∞§‡±Å‡∞§‡∞Ç ‡∞Ø‡±Å‡∞ï‡±Ü‡∞ú‡∞ø ‡∞ï‡∞Ç‡∞™‡±Ü‡∞®‡±Ä‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.\"\n",
        "\n",
        "data=\"‡∞µ‡±Ä‡∞∞‡∞∞‡∞æ‡∞ò‡∞µ ‡∞∏‡±ç‡∞µ‡∞æ‡∞Æ‡∞ø ‡∞¶‡±á‡∞µ‡∞æ‡∞≤‡∞Ø‡∞Ç (‡∞§‡∞ø‡∞∞‡±Å‡∞µ‡∞≥‡±ç‡∞≤‡±Ç‡∞∞‡±Å) ‡∞§‡∞ø‡∞∞‡±Å‡∞µ‡∞≥‡±ç‡∞≥‡±Ç‡∞∞‡±Å ‡∞µ‡±Ä‡∞∞‡∞∞‡∞æ‡∞ò‡∞µ ‡∞∏‡±ç‡∞µ‡∞æ‡∞Æ‡∞ø ‡∞Ü‡∞≤‡∞Ø‡∞Ç (‡∞≤‡±á‡∞¶‡∞æ ‡∞µ‡±Ä‡∞∞‡∞∞‡∞æ‡∞ò‡∞µ‡∞∏‡±ç‡∞µ‡∞æ‡∞Æ‡∞ø ‡∞Ü‡∞≤‡∞Ø‡∞Ç) ‡∞á‡∞¶‡∞ø ‡∞π‡∞ø‡∞Ç‡∞¶‡±Ç ‡∞¶‡±á‡∞µ‡∞§ ‡∞µ‡∞ø‡∞∑‡±ç‡∞£‡±Å‡∞µ‡±Å‡∞ï‡±Å ‡∞Ö‡∞Ç‡∞ï‡∞ø‡∞§‡∞Ç ‡∞ö‡±á‡∞∏‡∞ø‡∞® ‡∞Ü‡∞≤‡∞Ø‡∞Ç. ‡∞á‡∞¶‡∞ø ‡∞§‡∞Æ‡∞ø‡∞≥‡∞®‡∞æ‡∞°‡±Å ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç, ‡∞§‡∞ø‡∞∞‡±Å‡∞µ‡∞≥‡±ç‡∞≥‡±Ç‡∞∞‡±ç ‡∞ú‡∞ø‡∞≤‡±ç‡∞≤‡∞æ, ‡∞§‡∞ø‡∞∞‡±Å‡∞µ‡∞≥‡±ç‡∞≤‡±Ç‡∞∞‡±ç ‡∞®‡∞ó‡∞∞‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞¶‡±ç‡∞∞‡∞æ‡∞µ‡∞ø‡∞° ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞æ‡∞£ ‡∞∂‡±à‡∞≤‡∞ø‡∞≤‡±ã ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞® ‡∞à ‡∞Ü‡∞≤‡∞Ø‡∞Ç ‡∞∏‡∞æ.‡∞∂. 6‚Äì9‡∞µ ‡∞∂‡∞§‡∞æ‡∞¨‡±ç‡∞¶‡∞æ‡∞≤ ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞Ü‡∞≥‡±ç‡∞µ‡∞æ‡∞∞‡±ç ‡∞∏‡∞æ‡∞ß‡±Å‡∞µ‡±Å‡∞≤ ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠ ‡∞Æ‡∞ß‡±ç‡∞Ø‡∞Ø‡±Å‡∞ó ‡∞§‡∞Æ‡∞ø‡∞≥ ‡∞∂‡∞æ‡∞∏‡∞®‡∞Ç ‡∞Ö‡∞Ø‡∞ø‡∞® ‡∞¶‡∞ø‡∞µ‡±ç‡∞Ø ‡∞™‡±ç‡∞∞‡∞¨‡∞Ç‡∞ß‡∞Ç‡∞≤‡±ã ‡∞ï‡±Ä‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø. ‡∞µ‡∞ø‡∞∑‡±ç‡∞£‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø‡∞ï‡∞ø ‡∞Ö‡∞Ç‡∞ï‡∞ø‡∞§‡∞Ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞® 108 ‡∞¶‡∞ø‡∞µ‡±ç‡∞Ø‡∞¶‡±á‡∞∂‡∞æ‡∞≤‡∞≤‡±ã ‡∞á‡∞¶‡∞ø ‡∞í‡∞ï‡∞ü‡∞ø‡∞ó‡∞æ ‡∞™‡∞∞‡∞ø‡∞ó‡∞£‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø. ‡∞µ‡∞ø‡∞∑‡±ç‡∞£‡±Å‡∞µ‡±Å‡∞®‡±Å ‡∞µ‡±Ä‡∞∞‡∞∞‡∞æ‡∞ò‡∞µ ‡∞™‡±Ü‡∞∞‡±Å‡∞Æ‡∞æ‡∞≥‡±ç‚Äå‡∞ó‡∞æ‡∞®‡±Ç, ‡∞Ö‡∞§‡∞®‡∞ø ‡∞≠‡∞æ‡∞∞‡±ç‡∞Ø ‡∞≤‡∞ï‡±ç‡∞∑‡±ç‡∞Æ‡∞ø‡∞¶‡±á‡∞µ‡∞ø‡∞®‡∞ø ‡∞ï‡∞®‡∞ï‡∞µ‡∞≤‡±ç‡∞≤‡∞ø ‡∞§‡∞æ‡∞Ø‡∞∞‡±ç‚Äå‡∞ó‡∞æ‡∞®‡±Ç ‡∞™‡±Ç‡∞ú‡∞ø‡∞∏‡±ç‡∞§‡∞æ‡∞∞‡±Å. ‡∞à ‡∞Ü‡∞≤‡∞Ø‡∞Ç ‡∞∏‡∞æ.‡∞∂. 8‡∞µ ‡∞∂‡∞§‡∞æ‡∞¨‡±ç‡∞¶‡∞Ç ‡∞ö‡∞ø‡∞µ‡∞∞‡∞ø‡∞≤‡±ã ‡∞™‡∞≤‡±ç‡∞≤‡∞µ‡±Å‡∞≤‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡∞®‡∞ø ‡∞®‡∞Æ‡±ç‡∞Æ‡±Å‡∞§‡∞æ‡∞∞‡±Å. ‡∞§‡∞∞‡±Å‡∞µ‡∞æ‡∞§ ‡∞§‡∞Ç‡∞ú‡∞æ‡∞µ‡±Ç‡∞∞‡±Å ‡∞®‡∞æ‡∞Ø‡∞ï‡±Å‡∞≤‡±Å ‡∞µ‡∞ø‡∞µ‡∞ø‡∞ß ‡∞∏‡∞Æ‡∞Ø‡∞æ‡∞≤‡∞≤‡±ã‡∞à ‡∞Ü‡∞≤‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞µ‡∞ø‡∞∞‡∞æ‡∞≥‡∞æ‡∞≤‡±Å ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞Ü‡∞≤‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞ö‡±ã‡∞≥‡±Å‡∞≤ ‡∞ï‡∞æ‡∞≤‡∞Ç ‡∞®‡∞æ‡∞ü‡∞ø ‡∞Æ‡±Ç‡∞°‡±Å ‡∞∂‡∞æ‡∞∏‡∞®‡∞æ‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞à ‡∞Ü‡∞≤‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞è‡∞°‡±Å ‡∞Ö‡∞Ç‡∞ö‡±Ü‡∞≤ ‡∞∞‡∞æ‡∞ú‡∞ó‡±ã‡∞™‡±Å‡∞∞‡∞Ç (‡∞ó‡±á‡∞ü‡±ç‚Äå‡∞µ‡±á ‡∞ü‡∞µ‡∞∞‡±ç) ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞ó‡±ç‡∞∞‡∞æ‡∞®‡±à‡∞ü‡±ç ‡∞ó‡±ã‡∞°‡∞≤‡±ã ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø.‡∞à ‡∞∏‡∞Æ‡±Å‡∞¶‡∞æ‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞Ö‡∞®‡±ç‡∞®‡∞ø ‡∞™‡±Å‡∞£‡±ç‡∞Ø‡∞ï‡±ç‡∞∑‡±á‡∞§‡±ç‡∞∞‡∞æ‡∞≤‡±Å ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞Ü‡∞≤‡∞Ø‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡∞∂‡±ç‡∞ö‡∞ø‡∞Æ‡∞æ‡∞® ‡∞â‡∞®‡±ç‡∞® ‡∞π‡±É‡∞§‡±ç‡∞§‡∞™‡∞®‡∞æ‡∞∂‡∞ø‡∞®‡∞ø ‡∞Ö‡∞®‡∞ø ‡∞™‡∞ø‡∞≤‡∞ø‡∞ö‡±á ‡∞ï‡±ã‡∞®‡±á‡∞∞‡±Å ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞Ü‡∞≤‡∞Ø‡∞Ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡±á ‡∞í‡∞ï ‡∞ó‡±ã‡∞∂‡∞æ‡∞≤ ‡∞â‡∞Ç‡∞¶‡∞ø.\"\n",
        "\n",
        "from nltk.tokenize.punkt import PunktTrainer,PunktSentenceTokenizer\n",
        "\n",
        "trainer=PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS=True\n",
        "trainer.train(data)\n",
        "\n",
        "tokenizer=PunktSentenceTokenizer(trainer.get_params())\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPsO0_Kxr8VW",
        "outputId": "c6c6e981-2fe5-4058-9f45-293340b4ebe8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡∞®‡∞æ ‡∞™‡±á‡∞∞‡±Å ‡∞ö‡∞∞‡∞ø‡∞∑‡±ç‡∞Æ‡∞æ.',\n",
              " '‡∞®‡±á‡∞®‡±Å ‡∞Ü‡∞Ç‡∞ß‡±ç‡∞∞‡∞™‡±ç‡∞∞‡∞¶‡±á‡∞∂‡±ç\\u200c‡∞≤‡±ã‡∞®‡∞ø ‡∞®‡±Ü‡∞≤‡±ç‡∞≤‡±Ç‡∞∞‡±Å ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞µ‡∞ö‡±ç‡∞ö‡∞æ‡∞®‡±Å.',\n",
              " '‡∞™‡±ç‡∞∞‡∞∏‡±ç‡∞§‡±Å‡∞§‡∞Ç ‡∞Ø‡±Å‡∞ï‡±Ü‡∞ú‡∞ø ‡∞ï‡∞Ç‡∞™‡±Ü‡∞®‡±Ä‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbIAcD67r8R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7mWzW3jr8Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVq06yO4r8M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tK_9tprMr8KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "\n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAyres7suO1y",
        "outputId": "24bfdffc-f806-4147-db78-92770b81dda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Smith graduated from Harvard in 1999.',\n",
              " 'Can you believe it?',\n",
              " \"He said, 'I'm going to change the world!'\",\n",
              " 'And indeed, he did.',\n",
              " \"Now he's known globally‚Äîas a leader, a thinker, and an innovator.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_text = \"\"\"\n",
        "The sun rose over the quiet town. Birds chirped as morning light filtered through the trees.\n",
        "Dr. Allen stepped out of his home, carrying a cup of coffee. \"Another beautiful day,\" he muttered.\n",
        "Across the street, Mrs. Lewis waved cheerfully. The postman, arriving on his bicycle, greeted them both.\n",
        "By 10 a.m., the town square was bustling with activity‚Äîvendors, children, and the smell of fresh bread.\n",
        "Was this the calm before the storm? Only time would tell.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "# train on custom text\n",
        "trainer = PunktTrainer()\n",
        "trainer.train(custom_text)\n",
        "\n",
        "# create tokenizer from trained parameters\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n"
      ],
      "metadata": {
        "id": "39y-94A-uOyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Tokenization"
      ],
      "metadata": {
        "id": "YJO4ex9PvkFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4HO88-6wAsS",
        "outputId": "3dad8d2d-0755-44dd-ef76-6b027064953a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text=\"Hello everyone. Welcome to GeeksforGeeks .\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6gWz2yTuOwC",
        "outputId": "68cd3d75-2223-43d4-cdab-e3d6e50ca4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA1_rkzquOtW",
        "outputId": "3b84a6e8-7809-46bb-eaec-79a5df5dfbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "TweetTokenizer().tokenize(\"Hi there üòä! #happyday @user\")\n"
      ],
      "metadata": {
        "id": "AuXVziivuOrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25477dc-250f-4fc2-c5a6-9575d63619e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'there', 'üòä', '!', '#happyday', '@user']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import TweetTokenizer() method from nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create a reference variable for Class TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "\n",
        "# Create a string input\n",
        "gfg = \"Geeks for Geeks\"\n",
        "\n",
        "# Use tokenize method\n",
        "tk.tokenize(gfg)\n"
      ],
      "metadata": {
        "id": "ERLcxsmmuOol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cea4cb-7dc0-45d5-8f18-3daed8d06341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Geeks', 'for', 'Geeks']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tweet Tokenizer"
      ],
      "metadata": {
        "id": "mxsS47pJ0b_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create an instance\n",
        "tweet_tokenizer = TweetTokenizer()\n"
      ],
      "metadata": {
        "id": "1DocS5tYuOmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Example 1: Basic Tweet with Hashtags and Mentions"
      ],
      "metadata": {
        "id": "WF0Js2Z-0jPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Loving the weather today! #sunnyday @weatherchannel\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "s6MDuLrbuOje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0bce7a-bd2b-4317-e908-a0a81a074dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Loving', 'the', 'weather', 'today', '!', '#sunnyday', '@weatherchannel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Emojis and Casual Text"
      ],
      "metadata": {
        "id": "yc5EKNVL0oss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I'm sooo happy üòÑüòÑüòÑ!! Can't wait for the weekend.\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "sCUOsUBKuOhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e8070d-77bd-46d3-cff8-c1ed50057c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I'm\", 'sooo', 'happy', 'üòÑ', 'üòÑ', 'üòÑ', '!', '!', \"Can't\", 'wait', 'for', 'the', 'weekend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: URLs and Abbreviations"
      ],
      "metadata": {
        "id": "g9YdHA0f0zuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Check out our site: https://example.com lol üòÇ\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "kYlt98JGuOeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81662a06-89a2-42a0-f94f-e6dfa5e73046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Check', 'out', 'our', 'site', ':', 'https://example.com', 'lol', 'üòÇ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Repeated Characters & Punctuation"
      ],
      "metadata": {
        "id": "ZtIAp3s304xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Nooooo!!! Why did this happen?!?!\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ug8cKgw02pu",
        "outputId": "2f4bb405-ef4a-4c23-c8aa-1ca56f7a440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nooooo', '!', '!', '!', 'Why', 'did', 'this', 'happen', '?', '!', '?', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Tokenize With Reduce Length and Strip Handles"
      ],
      "metadata": {
        "id": "1CYQuk_s16Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "text = \"@user Hellooooooo!!!! I'm sooooooo tired üò©\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPXVvwlr02mc",
        "outputId": "9191e00b-c1da-4cb6-9e2a-77809b9bbbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hellooo', '!', '!', '!', \"I'm\", 'sooo', 'tired', 'üò©']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of What TweetTokenizer Handles Well"
      ],
      "metadata": {
        "id": "k0YfFeKc2F5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                            | Handled?                   |\n",
        "| ---------------------------------- | -------------------------- |\n",
        "| Mentions (`@user`)                 | ‚úÖ                          |\n",
        "| Hashtags (`#tag`)                  | ‚úÖ                          |\n",
        "| URLs                               | ‚úÖ                          |\n",
        "| Emojis                             | ‚úÖ                          |\n",
        "| Repeated punctuation (`!!!`, `?!`) | ‚úÖ                          |\n",
        "| Contractions (`I'm`, `don't`)      | ‚úÖ                          |\n",
        "| Casual text (slang, misspellings)  | ‚úÖ                          |\n",
        "| Repeated characters                | ‚úÖ (with `reduce_len=True`) |\n"
      ],
      "metadata": {
        "id": "OAJt8gjg2Cc0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZWTfHUt02j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scAJ7uSP02hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqGrt-yv02e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0sq1uerV02cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, TweetTokenizer considers hashtag along with the words as one token while word_tokenize splits it into 2 different tokens. Similarly with username and emoji in the corpus."
      ],
      "metadata": {
        "id": "pMQVPXIC1s71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:)\"\n",
        "tokenizer=TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "metadata": {
        "id": "Q2GC0WH0uOcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41361054-2569-4449-9b38-78e8174ab90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s=\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:)\"\n",
        "word_tokenize(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SUP1kzT1n1L",
        "outputId": "9bf414ec-0912-4fbd-8ddb-ad4c0cad7a80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Snow',\n",
              " 'White',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Seven',\n",
              " 'Degrees',\n",
              " '#',\n",
              " 'MakeAMovieCold',\n",
              " '@',\n",
              " 'midnight',\n",
              " ':',\n",
              " ')']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eODDnbDy1nxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Y5YmOG8pyDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NN8eBxxSpx_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkBTF3wEpx9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdFMQfhBpx6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fx3VXNXpx4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "d5exxwSA1nvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1d211c-5120-407c-b1e7-5531b587da63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM5cOovaeU9l",
        "outputId": "35a17abb-f33e-429d-8e0a-d62702a27178"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"Hello, Good morning!!! How was your day? Is everything fine.\"\n",
        "\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "tWWXM5Fvd0i1",
        "outputId": "d4df729d-c50d-49ee-cdda-53770586ebe0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9758f6a43dd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hello, Good morning!!! How was your day? Is everything fine.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import load\n",
        "\n",
        "tokenizer=load('tokenizers/punkt/PY3/english.pickle')\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD4T57Ktd0XE",
        "outputId": "ee5f0f1b-ff6e-4e43-f21b-641b9b4eaf89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, Good morning!!!', 'How was your day?', 'Is everything fine.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_04Z7BHd0SU",
        "outputId": "4d8b5d6e-0670-4beb-e6b2-4ca88e2788f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Good',\n",
              " 'morning',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " 'How',\n",
              " 'was',\n",
              " 'your',\n",
              " 'day',\n",
              " '?',\n",
              " 'Is',\n",
              " 'everything',\n",
              " 'fine',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "tokenizer=PunktSentenceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYYBGP26d0Mu",
        "outputId": "08a816e2-9142-4c29-b99a-3356f659f781"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, Good morning!!!', 'How was your day?', 'Is everything fine.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTbrlh6md0Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOTyKmkHd0DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjZT2eIJdz_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFTaJNB9dz60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-W9na45qdz2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGRThzDpdzxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uoOdaf_ndzsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OS9wvrNxdznP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzyKKBrjdzid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y_ImMZ1dzeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cPzE9qgdzWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNwSaX8BdzMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2MOiLFGdzIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vk4VcZuHdzE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7tHPjcTdzBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pskOLqAVdy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6T1a-vkydy1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Pg6Vaoudyym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rFNe-LJdyvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOLYiBNEdysM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UB37aPDVdyo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7qrupq7dyi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ur1Ig9Xdyc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgSbFMBndyYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KW8N1q2RdyVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ltu_4eTCdyRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSDstK5LdyOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4AqFNlJdyK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y38AaqDMdyCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fokIxLl-dx8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T24eVKUC1ns2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}