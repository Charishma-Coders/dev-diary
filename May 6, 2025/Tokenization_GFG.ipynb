{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzR_IsxIrBbL",
        "outputId": "7bead19d-3413-434b-c8eb-1700c4c1f9db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLqTzjPHrSnh",
        "outputId": "e99d9086-26c8-4e23-ba0b-b7cd1f6b6cd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZbpv3vKqILD",
        "outputId": "c8781586-d081-49c3-9752-ccd3d52dba23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Smith graduated from Harvard in 1999.',\n",
              " 'Can you believe it?',\n",
              " \"He said, 'I'm going to change the world!'\",\n",
              " 'And indeed, he did.',\n",
              " \"Now he's known globally—as a leader, a thinker, and an innovator.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"\"\"Dr. Smith graduated from Harvard in 1999. Can you believe it?\n",
        "He said, 'I'm going to change the world!' And indeed, he did.\n",
        "Now he's known globally—as a leader, a thinker, and an innovator.\n",
        "\"\"\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to train a PunkTrainer to train in Telugu."
      ],
      "metadata": {
        "id": "LR04rVMQtkvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.punkt import PunktTrainer, PunktSentenceTokenizer\n",
        "\n",
        "# Step 1: Prepare training text (Telugu)\n",
        "telugu_text = \"\"\"\n",
        "తెలంగాణ రాష్ట్రం భారతదేశంలోని 29 రాష్ట్రాలలో ఒకటి.\n",
        "ఇది దక్షిణ భారతదేశంలో ఉంది. హైదరాబాద్ నగరం ఈ రాష్ట్ర రాజధాని.\n",
        "తెలంగాణ అనేది భారతదేశంలో పదవ అతి పెద్ద రాష్ట్రం.\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Train a Punkt tokenizer\n",
        "trainer = PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS = True\n",
        "trainer.train(telugu_text)\n",
        "\n",
        "print(trainer.get_params())\n",
        "\n",
        "# Step 3: Create tokenizer from the trainer\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
        "\n",
        "# Step 4: Tokenize Telugu text\n",
        "sentences = tokenizer.tokenize(telugu_text)\n",
        "\n",
        "# Output\n",
        "for i, s in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {s}\")\n",
        "\n",
        "random_text=\"భారతదేశం ఒక విశాలమైన దేశం. ఇది భిన్నమైన భాషలు, సంస్కృతులు కలిగి ఉంది. హిమాలయాలు ఉత్తరంలో ఉన్నాయి. గంగా నది గొప్ప నది. ఇది ఎంతో పవిత్రంగా పరిగణించబడుతుంది.\"\n",
        "\n",
        "tokenizer.tokenize(random_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84SearfsaPO",
        "outputId": "f69a00f0-72de-4e5c-b898-cc7086cc9b96",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<nltk.tokenize.punkt.PunktParameters object at 0x79aefa28d910>\n",
            "Sentence 1: \n",
            "తెలంగాణ రాష్ట్రం భారతదేశంలోని 29 రాష్ట్రాలలో ఒకటి.\n",
            "Sentence 2: ఇది దక్షిణ భారతదేశంలో ఉంది.\n",
            "Sentence 3: హైదరాబాద్ నగరం ఈ రాష్ట్ర రాజధాని.\n",
            "Sentence 4: తెలంగాణ అనేది భారతదేశంలో పదవ అతి పెద్ద రాష్ట్రం.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['భారతదేశం ఒక విశాలమైన దేశం.',\n",
              " 'ఇది భిన్నమైన భాషలు, సంస్కృతులు కలిగి ఉంది.',\n",
              " 'హిమాలయాలు ఉత్తరంలో ఉన్నాయి.',\n",
              " 'గంగా నది గొప్ప నది.',\n",
              " 'ఇది ఎంతో పవిత్రంగా పరిగణించబడుతుంది.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"నా పేరు చరిష్మా. నేను ఆంధ్రప్రదేశ్‌లోని నెల్లూరు నుండి వచ్చాను. ప్రస్తుతం యుకెజి కంపెనీలో పనిచేస్తున్నాను.\"\n",
        "\n",
        "data=\"వీరరాఘవ స్వామి దేవాలయం (తిరువళ్లూరు) తిరువళ్ళూరు వీరరాఘవ స్వామి ఆలయం (లేదా వీరరాఘవస్వామి ఆలయం) ఇది హిందూ దేవత విష్ణువుకు అంకితం చేసిన ఆలయం. ఇది తమిళనాడు రాష్ట్రం, తిరువళ్ళూర్ జిల్లా, తిరువళ్లూర్ నగరంలో ఉంది. ద్రావిడ నిర్మాణ శైలిలో నిర్మించబడిన ఈ ఆలయం సా.శ. 6–9వ శతాబ్దాల నుండి ఆళ్వార్ సాధువుల ప్రారంభ మధ్యయుగ తమిళ శాసనం అయిన దివ్య ప్రబంధంలో కీర్తించబడింది. విష్ణుమూర్తికి అంకితం చేయబడిన 108 దివ్యదేశాలలో ఇది ఒకటిగా పరిగణించబడుతుంది. విష్ణువును వీరరాఘవ పెరుమాళ్‌గానూ, అతని భార్య లక్ష్మిదేవిని కనకవల్లి తాయర్‌గానూ పూజిస్తారు. ఈ ఆలయం సా.శ. 8వ శతాబ్దం చివరిలో పల్లవులు ప్రారంభించారని నమ్ముతారు. తరువాత తంజావూరు నాయకులు వివిధ సమయాలలోఈ ఆలయానికి విరాళాలు అందించారు. ఈ ఆలయంలో చోళుల కాలం నాటి మూడు శాసనాలు ఉన్నాయి. ఈ ఆలయంలో ఏడు అంచెల రాజగోపురం (గేట్‌వే టవర్) ఉంది. గ్రానైట్ గోడలో ప్రతిష్టించబడింది.ఈ సముదాయంలో అన్ని పుణ్యక్షేత్రాలు ఉన్నాయి. ఆలయానికి పశ్చిమాన ఉన్న హృత్తపనాశిని అని పిలిచే కోనేరు ఉంది. ఆలయం నిర్వహించే ఒక గోశాల ఉంది.\"\n",
        "\n",
        "from nltk.tokenize.punkt import PunktTrainer,PunktSentenceTokenizer\n",
        "\n",
        "trainer=PunktTrainer()\n",
        "trainer.INCLUDE_ALL_COLLOCS=True\n",
        "trainer.train(data)\n",
        "\n",
        "tokenizer=PunktSentenceTokenizer(trainer.get_params())\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPsO0_Kxr8VW",
        "outputId": "c6c6e981-2fe5-4058-9f45-293340b4ebe8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['నా పేరు చరిష్మా.',\n",
              " 'నేను ఆంధ్రప్రదేశ్\\u200cలోని నెల్లూరు నుండి వచ్చాను.',\n",
              " 'ప్రస్తుతం యుకెజి కంపెనీలో పనిచేస్తున్నాను.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbIAcD67r8R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7mWzW3jr8Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVq06yO4r8M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tK_9tprMr8KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "\n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAyres7suO1y",
        "outputId": "24bfdffc-f806-4147-db78-92770b81dda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. Smith graduated from Harvard in 1999.',\n",
              " 'Can you believe it?',\n",
              " \"He said, 'I'm going to change the world!'\",\n",
              " 'And indeed, he did.',\n",
              " \"Now he's known globally—as a leader, a thinker, and an innovator.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_text = \"\"\"\n",
        "The sun rose over the quiet town. Birds chirped as morning light filtered through the trees.\n",
        "Dr. Allen stepped out of his home, carrying a cup of coffee. \"Another beautiful day,\" he muttered.\n",
        "Across the street, Mrs. Lewis waved cheerfully. The postman, arriving on his bicycle, greeted them both.\n",
        "By 10 a.m., the town square was bustling with activity—vendors, children, and the smell of fresh bread.\n",
        "Was this the calm before the storm? Only time would tell.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
        "\n",
        "# train on custom text\n",
        "trainer = PunktTrainer()\n",
        "trainer.train(custom_text)\n",
        "\n",
        "# create tokenizer from trained parameters\n",
        "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n"
      ],
      "metadata": {
        "id": "39y-94A-uOyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Tokenization"
      ],
      "metadata": {
        "id": "YJO4ex9PvkFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4HO88-6wAsS",
        "outputId": "3dad8d2d-0755-44dd-ef76-6b027064953a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text=\"Hello everyone. Welcome to GeeksforGeeks .\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6gWz2yTuOwC",
        "outputId": "68cd3d75-2223-43d4-cdab-e3d6e50ca4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA1_rkzquOtW",
        "outputId": "3b84a6e8-7809-46bb-eaec-79a5df5dfbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "TweetTokenizer().tokenize(\"Hi there 😊! #happyday @user\")\n"
      ],
      "metadata": {
        "id": "AuXVziivuOrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25477dc-250f-4fc2-c5a6-9575d63619e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'there', '😊', '!', '#happyday', '@user']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import TweetTokenizer() method from nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create a reference variable for Class TweetTokenizer\n",
        "tk = TweetTokenizer()\n",
        "\n",
        "# Create a string input\n",
        "gfg = \"Geeks for Geeks\"\n",
        "\n",
        "# Use tokenize method\n",
        "tk.tokenize(gfg)\n"
      ],
      "metadata": {
        "id": "ERLcxsmmuOol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cea4cb-7dc0-45d5-8f18-3daed8d06341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Geeks', 'for', 'Geeks']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tweet Tokenizer"
      ],
      "metadata": {
        "id": "mxsS47pJ0b_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Create an instance\n",
        "tweet_tokenizer = TweetTokenizer()\n"
      ],
      "metadata": {
        "id": "1DocS5tYuOmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Example 1: Basic Tweet with Hashtags and Mentions"
      ],
      "metadata": {
        "id": "WF0Js2Z-0jPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Loving the weather today! #sunnyday @weatherchannel\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "s6MDuLrbuOje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0bce7a-bd2b-4317-e908-a0a81a074dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Loving', 'the', 'weather', 'today', '!', '#sunnyday', '@weatherchannel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Emojis and Casual Text"
      ],
      "metadata": {
        "id": "yc5EKNVL0oss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I'm sooo happy 😄😄😄!! Can't wait for the weekend.\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "sCUOsUBKuOhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e8070d-77bd-46d3-cff8-c1ed50057c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I'm\", 'sooo', 'happy', '😄', '😄', '😄', '!', '!', \"Can't\", 'wait', 'for', 'the', 'weekend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: URLs and Abbreviations"
      ],
      "metadata": {
        "id": "g9YdHA0f0zuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Check out our site: https://example.com lol 😂\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "kYlt98JGuOeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81662a06-89a2-42a0-f94f-e6dfa5e73046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Check', 'out', 'our', 'site', ':', 'https://example.com', 'lol', '😂']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 4: Repeated Characters & Punctuation"
      ],
      "metadata": {
        "id": "ZtIAp3s304xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Nooooo!!! Why did this happen?!?!\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ug8cKgw02pu",
        "outputId": "2f4bb405-ef4a-4c23-c8aa-1ca56f7a440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nooooo', '!', '!', '!', 'Why', 'did', 'this', 'happen', '?', '!', '?', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 5: Tokenize With Reduce Length and Strip Handles"
      ],
      "metadata": {
        "id": "1CYQuk_s16Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "\n",
        "text = \"@user Hellooooooo!!!! I'm sooooooo tired 😩\"\n",
        "tokens = tweet_tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPXVvwlr02mc",
        "outputId": "9191e00b-c1da-4cb6-9e2a-77809b9bbbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hellooo', '!', '!', '!', \"I'm\", 'sooo', 'tired', '😩']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of What TweetTokenizer Handles Well"
      ],
      "metadata": {
        "id": "k0YfFeKc2F5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                            | Handled?                   |\n",
        "| ---------------------------------- | -------------------------- |\n",
        "| Mentions (`@user`)                 | ✅                          |\n",
        "| Hashtags (`#tag`)                  | ✅                          |\n",
        "| URLs                               | ✅                          |\n",
        "| Emojis                             | ✅                          |\n",
        "| Repeated punctuation (`!!!`, `?!`) | ✅                          |\n",
        "| Contractions (`I'm`, `don't`)      | ✅                          |\n",
        "| Casual text (slang, misspellings)  | ✅                          |\n",
        "| Repeated characters                | ✅ (with `reduce_len=True`) |\n"
      ],
      "metadata": {
        "id": "OAJt8gjg2Cc0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZWTfHUt02j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scAJ7uSP02hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqGrt-yv02e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0sq1uerV02cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, TweetTokenizer considers hashtag along with the words as one token while word_tokenize splits it into 2 different tokens. Similarly with username and emoji in the corpus."
      ],
      "metadata": {
        "id": "pMQVPXIC1s71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet=u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:)\"\n",
        "tokenizer=TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "metadata": {
        "id": "Q2GC0WH0uOcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41361054-2569-4449-9b38-78e8174ab90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s=\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:)\"\n",
        "word_tokenize(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SUP1kzT1n1L",
        "outputId": "9bf414ec-0912-4fbd-8ddb-ad4c0cad7a80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Snow',\n",
              " 'White',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Seven',\n",
              " 'Degrees',\n",
              " '#',\n",
              " 'MakeAMovieCold',\n",
              " '@',\n",
              " 'midnight',\n",
              " ':',\n",
              " ')']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eODDnbDy1nxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Y5YmOG8pyDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NN8eBxxSpx_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkBTF3wEpx9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdFMQfhBpx6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fx3VXNXpx4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "d5exxwSA1nvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1d211c-5120-407c-b1e7-5531b587da63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM5cOovaeU9l",
        "outputId": "35a17abb-f33e-429d-8e0a-d62702a27178"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text=\"Hello, Good morning!!! How was your day? Is everything fine.\"\n",
        "\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "tWWXM5Fvd0i1",
        "outputId": "d4df729d-c50d-49ee-cdda-53770586ebe0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9758f6a43dd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hello, Good morning!!! How was your day? Is everything fine.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.data import load\n",
        "\n",
        "tokenizer=load('tokenizers/punkt/PY3/english.pickle')\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD4T57Ktd0XE",
        "outputId": "ee5f0f1b-ff6e-4e43-f21b-641b9b4eaf89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, Good morning!!!', 'How was your day?', 'Is everything fine.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_04Z7BHd0SU",
        "outputId": "4d8b5d6e-0670-4beb-e6b2-4ca88e2788f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Good',\n",
              " 'morning',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " 'How',\n",
              " 'was',\n",
              " 'your',\n",
              " 'day',\n",
              " '?',\n",
              " 'Is',\n",
              " 'everything',\n",
              " 'fine',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "tokenizer=PunktSentenceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYYBGP26d0Mu",
        "outputId": "08a816e2-9142-4c29-b99a-3356f659f781"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello, Good morning!!!', 'How was your day?', 'Is everything fine.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTbrlh6md0Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOTyKmkHd0DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjZT2eIJdz_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFTaJNB9dz60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-W9na45qdz2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGRThzDpdzxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uoOdaf_ndzsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OS9wvrNxdznP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzyKKBrjdzid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y_ImMZ1dzeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cPzE9qgdzWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNwSaX8BdzMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2MOiLFGdzIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vk4VcZuHdzE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7tHPjcTdzBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pskOLqAVdy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6T1a-vkydy1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Pg6Vaoudyym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rFNe-LJdyvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOLYiBNEdysM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UB37aPDVdyo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7qrupq7dyi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ur1Ig9Xdyc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgSbFMBndyYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KW8N1q2RdyVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ltu_4eTCdyRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSDstK5LdyOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4AqFNlJdyK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y38AaqDMdyCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fokIxLl-dx8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T24eVKUC1ns2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}